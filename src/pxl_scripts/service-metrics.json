{
    "name": "Service Metrics",
    "description": "Use with the Table visualization. Displays metrics about each service in the cluster.",
    "script": "# $pixieCluster - work around to update the panel if this dashboard variable is present\n\nimport px\n\nfilter_health_checks = True\n\ndef get_time_window(start_time: int):\n    ''' Converts the start_time string into a table with a single column and single row.\n    The approach is hacky, and will round to roughly 1 second.\n    '''\n    df = px.DataFrame('process_stats', start_time=start_time)\n\n    df = df.agg(\n        time_min=('time_', px.min),\n        time_max=('time_', px.max),\n    )\n\n    df.window = px.DurationNanos(df.time_max - df.time_min)\n    df = df[['window']]\n\n    return df\n    \ndef add_time_window_column(df, start_time):\n    tw = get_time_window(start_time)\n    df = df.merge(tw, how='inner', left_on=[], right_on=[])\n    return df\n    \ndef http_stats(start_time: int):\n    ''' Get a dataframe of HTTP events.\n    Certain traffic (like health checks) are auto removed, and some standard fields are added.\n    Args:\n    @start_time: The timestamp of data to start at.\n    '''\n    df = px.DataFrame(table='http_events', start_time=start_time)\n\n    # Add K8s metadata.\n    df.service = df.ctx['service']\n    df.pod = df.ctx['pod']\n\n    # Filter out non-k8s entities.\n    df = df[df.pod != '']\n\n    # Additional HTTP fields, pre-computed for convenience.\n    df.failure = df.resp_status >= 400\n\n    # Remove health checks, and anything with no remote address.\n    health_check_req = ((df.req_path == '/healthz' or df.req_path == '/readyz') or df.req_path == '/livez')\n    filter_out_conds = (health_check_req and filter_health_checks) or (df['remote_addr'] == '-')\n    df = df[not filter_out_conds]\n\n    return df\n    \ndef conn_stats(start_time: int):\n    ''' Get a dataframe of connection stats.\n    For each client-server pair, the resulting data frame has the bytes sent and received.\n    Args:\n    @start_time: The timestamp of data to start at.\n    '''\n    df = px.DataFrame(table='conn_stats', start_time=start_time)\n\n    df.pod = df.ctx['pod']\n    df.service = df.ctx['service']\n\n    df = df[df.service != '']\n\n    # Find min/max bytes transferred over the selected time window per pod.\n    df = df.groupby(['upid', 'remote_addr', 'remote_port', 'pod', 'service', 'trace_role']).agg(\n        bytes_recv_min=('bytes_recv', px.min),\n        bytes_recv_max=('bytes_recv', px.max),\n        bytes_sent_min=('bytes_sent', px.min),\n        bytes_sent_max=('bytes_sent', px.max),\n    )\n\n    # Calculate bytes transferred over the time window\n    df.bytes_sent = df.bytes_sent_max - df.bytes_sent_min\n    df.bytes_recv = df.bytes_recv_max - df.bytes_recv_min\n    df = df.drop(['bytes_recv_min', 'bytes_recv_max', 'bytes_sent_min', 'bytes_sent_max'])\n\n    return df\n    \ndef http_stats_by_service(start_time: int):\n    ''' Get a data frame of HTTP stats per service. The HTTP stats are for inbound traffic,\n    and includes HTTP request count, error count and latency quantiles.\n    Args:\n    @start_time: The timestamp of data to start at.\n    '''\n    df = http_stats(start_time)\n\n    # Filter only to inbound service traffic (server-side).\n    # Don't include traffic initiated by this service to an external location.\n    df = df[df.trace_role == 2]\n\n    # Compute HTTP metrics.\n    df = df.groupby(['service']).agg(\n        http_req_count_in=('latency', px.count),\n        http_error_count_in=('failure', px.sum),\n        http_latency_in=('latency', px.quantiles)\n    )\n\n    return df\n    \ndef conn_stats_by_service(start_time: int):\n    ''' Get a dataframe of connection stats aggregated by service.\n    For each service, the resulting data frame contains rx/tx stats for server-side and client-side connections.\n    Args:\n    @start_time: The timestamp of data to start at.\n    '''\n    df = conn_stats(start_time)\n\n    # Group by service and trace role.\n    # Do this after computing bytes sent/received by conn_stats key ({upid, remote_addr, remote_port}).\n    # Keeping trace_role allows us to see which traffic was part of server duties vs client duties.\n    df = df.groupby(['service', 'trace_role']).agg(\n        bytes_recv=('bytes_recv', px.sum),\n        bytes_sent=('bytes_sent', px.sum),\n    )\n\n    # Get RX/TX stats for the server side connections.\n    server_df = df[df.trace_role == 2]\n    server_df.rx_server = server_df.bytes_recv\n    server_df.tx_server = server_df.bytes_sent\n    server_df = server_df[['service', 'rx_server', 'tx_server']]\n\n    # Get RX/TX stats for the client side connections.\n    client_df = df[df.trace_role == 1]\n    client_df.rx_client = client_df.bytes_recv\n    client_df.tx_client = client_df.bytes_sent\n    client_df = client_df[['service', 'rx_client', 'tx_client']]\n\n    # Create a dataframe that contains both server-side and client-side RX/TX stats.\n    df = server_df.merge(client_df,\n                         how='left',\n                         left_on='service',\n                         right_on='service',\n                         suffixes=['', '_x'])\n    df = df['service', 'rx_server', 'tx_server', 'rx_client', 'tx_client']\n\n    return df\n    \ndef service_let_summary(start_time: int):\n    ''' Compute a summary of traffic by requesting service, for requests\n        on services in the current cluster..\n    Args:\n    @start_time: The timestamp of data to start at.\n    '''\n    conn_stats_df = conn_stats_by_service(start_time)\n    http_stats_df = http_stats_by_service(start_time)\n\n    # Merge conn_stats_df and http_stats_df.\n    df = conn_stats_df.merge(http_stats_df,\n                             how='left',\n                             left_on='service',\n                             right_on='service',\n                             suffixes=['', '_x'])\n\n    # Compute time window for the query and add it as a column.\n    df = add_time_window_column(df, start_time)\n\n    # Compute throughput values.\n    df.http_req_throughput_in = df.http_req_count_in / df.window\n    df.http_error_rate_in = px.Percent(\n        px.select(df.http_req_count_in != 0, df.http_error_count_in / df.http_req_count_in, 0.0))\n    df.inbound_conns = (df.rx_server + df.tx_server) / df.window\n    df.outbound_conns = (df.tx_client + df.rx_client) / df.window\n\n    return df[['service', 'http_latency_in', 'http_req_throughput_in', 'http_error_rate_in',\n               'inbound_conns', 'outbound_conns']]\n               \n''' Compute a summary of traffic by requesting service, for requests\n    on services in the current cluster..\nArgs:\n@start_time: The timestamp of data to start at.\n'''\nconn_stats_df = conn_stats_by_service(start_time=$__from)\nhttp_stats_df = http_stats_by_service($__from)\n\n# Merge conn_stats_df and http_stats_df.\ndf = conn_stats_df.merge(http_stats_df,\n                         how='left',\n                         left_on='service',\n                         right_on='service',\n                         suffixes=['', '_x'])\n\n# Compute time window for the query and add it as a column.\ndf = add_time_window_column(df, $__from)\n\n# Compute throughput values.\ndf.http_req_throughput_in = df.http_req_count_in / df.window\ndf.http_error_rate_in = px.Percent(\n    px.select(df.http_req_count_in != 0, df.http_error_count_in / df.http_req_count_in, 0.0))\ndf.inbound_conns = (df.rx_server + df.tx_server) / df.window\ndf.outbound_conns = (df.tx_client + df.rx_client) / df.window\n\ndf = df[[$__columns]]\npx.display(df)",
    "columnNames": ["service", "http_latency_in", "http_req_throughput_in", "http_error_rate_in","inbound_conns", "outbound_conns"],
    "groupByColumns": ["service", "http_latency_in", "http_req_throughput_in", "http_error_rate_in","inbound_conns", "outbound_conns"],
    "isColDisplay": true,
    "isGroupBy": true
}
